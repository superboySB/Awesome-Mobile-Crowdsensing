# Copyright (c) 2021, salesforce.com, inc.
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# For full license text, see the LICENSE file in the repo root
# or https://opensource.org/licenses/BSD-3-Clause

# YAML configuration for the tag continuous environment
name: "tag_continuous"
# Environment settings
env:
    num_taggers: 10
    num_runners: 100
    grid_length: 20.0
    episode_length: 500
    max_acceleration: 0.1
    min_acceleration: -0.1
    max_turn: 2.356 # 3*pi/4 radians
    min_turn: -2.356 # -3*pi/4 radians
    num_acceleration_levels: 20
    num_turn_levels: 20
    skill_level_runner: 1.0
    skill_level_tagger: 1.0
    max_speed: 1.0
    seed: 274880
    use_full_observation: False
    runner_exits_game_after_tagged: True
    num_other_agents_observed: 10
    tag_reward_for_tagger: 10.0
    tag_penalty_for_runner: -10.0
    step_penalty_for_tagger: 0.0
    step_reward_for_runner: 0.0
    edge_hit_penalty: 0.0
    end_of_game_reward_for_runner: 1.0
    tagging_distance: 0.02
# Trainer settings
trainer:
    num_envs: 100 # number of environment replicas
    num_episodes: 500 # number of episodes to run the training for. Can be arbitrarily high!
    train_batch_size: 25000 # total batch size used for training per iteration (across all the environments)
# Policy network settings
policy: # list all the policies below
    runner:
        to_train: True # flag indicating whether the model needs to be trained
        algorithm: "A2C" # algorithm used to train the policy
        vf_loss_coeff: 1 # loss coefficient for the value function loss
        entropy_coeff: 0.05 # coefficient for the entropy component of the loss
        clip_grad_norm: True # flag indicating whether to clip the gradient norm or not
        max_grad_norm: 0.5 # when clip_grad_norm is True, the clip level
        normalize_advantage: False # flag indicating whether to normalize advantage or not
        normalize_return: False # flag indicating whether to normalize return or not
        gamma: 0.98 # discount factor
        lr: 0.005 # learning rate
        model: # policy model settings
            type: "fully_connected" # model type
            fc_dims: [256, 256] # dimension(s) of the fully connected layers as a list
            model_ckpt_filepath: "" # filepath (used to restore a previously saved model)
    tagger:
        to_train: True
        algorithm: "A2C"
        vf_loss_coeff: 1
        entropy_coeff: 0.05
        clip_grad_norm: True
        max_grad_norm: 0.5
        normalize_advantage: False
        normalize_return: False
        gamma: 0.98
        lr: 0.001
        model:
            type: "fully_connected"
            fc_dims: [256, 256]
            model_ckpt_filepath: ""
# Checkpoint saving setting
saving:
    metrics_log_freq: 100 # how often (in iterations) to log (and print) the metrics
    model_params_save_freq: 5000 # how often (in iterations) to save the model parameters
    basedir: "/tmp" # base folder used for saving
    name: "tag_continuous" # base folder used for saving
    tag: "experiments" # experiment name

