{"loss_runner": -2.1129672527313232, "VF loss coefficient_runner": 0.009999999776482582, "Entropy coefficient_runner": 0.05000000074505806, "Total loss_runner": -2.1129672527313232, "Policy loss_runner": -1.897732138633728, "Value function loss_runner": 2.4391050338745117, "Mean rewards_runner": -0.009949999861419201, "Max. rewards_runner": 1.0, "Min. rewards_runner": -10.0, "Mean value function_runner": -0.01885533519089222, "Mean advantages_runner": -0.39610177278518677, "Mean (norm.) advantages_runner": -0.39610177278518677, "Mean (discounted) returns_runner": -0.41495707631111145, "Mean normalized returns_runner": -0.41495707631111145, "Mean entropy_runner": 4.792523384094238, "Variance explained by the value function_runner": -0.0004336833953857422, "Std. of action_0 over agents_runner": 3.1819183826446533, "Std. of action_0 over envs_runner": 3.1777565479278564, "Std. of action_0 over time_runner": 3.1829466819763184, "Std. of action_1 over agents_runner": 3.1464624404907227, "Std. of action_1 over envs_runner": 3.1431336402893066, "Std. of action_1 over time_runner": 3.148071050643921, "Current timestep_runner": 2000000.0, "Gradient norm_runner": 0.0, "Mean episodic reward_runner": -666.2000122070312, "epoch": 199, "trainer/global_step": 199, "_timestamp": 1692857634.7099283, "_runtime": 122.91390538215637, "_step": 20, "_wandb": {"runtime": 145}}