{"loss_runner": -2.1769375801086426, "VF loss coefficient_runner": 0.009999999776482582, "Entropy coefficient_runner": 0.05000000074505806, "Total loss_runner": -2.1769375801086426, "Policy loss_runner": -1.972028374671936, "Value function loss_runner": 3.4701993465423584, "Mean rewards_runner": -0.025750000029802322, "Max. rewards_runner": 0.0, "Min. rewards_runner": -10.0, "Mean value function_runner": -0.10427744686603546, "Mean advantages_runner": -0.41148149967193604, "Mean (norm.) advantages_runner": -0.41148149967193604, "Mean (discounted) returns_runner": -0.5157589316368103, "Mean normalized returns_runner": -0.5157589316368103, "Mean entropy_runner": 4.7922210693359375, "Variance explained by the value function_runner": 0.0023573637008666992, "Std. of action_0 over agents_runner": 3.134066581726074, "Std. of action_0 over envs_runner": 3.1360092163085938, "Std. of action_0 over time_runner": 3.1308822631835938, "Std. of action_1 over agents_runner": 3.196160078048706, "Std. of action_1 over envs_runner": 3.197840929031372, "Std. of action_1 over time_runner": 3.1931569576263428, "Current timestep_runner": 16100000.0, "Gradient norm_runner": 0.0, "Mean episodic reward_runner": -693.375, "epoch": 1609, "trainer/global_step": 1609, "_timestamp": 1692858142.1931446, "_runtime": 417.45705366134644, "_step": 160, "_wandb": {"runtime": 442}}