{"loss_runner": -4.126214504241943, "VF loss coefficient_runner": 0.009999999776482582, "Entropy coefficient_runner": 0.05000000074505806, "Total loss_runner": -4.126214504241943, "Policy loss_runner": -3.9365804195404053, "Value function loss_runner": 5.001875877380371, "Mean rewards_runner": -0.021856000646948814, "Max. rewards_runner": 1.0, "Min. rewards_runner": -10.0, "Mean value function_runner": -0.004377500619739294, "Mean advantages_runner": -0.8213517665863037, "Mean (norm.) advantages_runner": -0.8213517665863037, "Mean (discounted) returns_runner": -0.8257291913032532, "Mean normalized returns_runner": -0.8257291913032532, "Mean entropy_runner": 4.793060779571533, "Variance explained by the value function_runner": -0.0029447078704833984, "Std. of action_0 over agents_runner": 3.1200530529022217, "Std. of action_0 over envs_runner": 3.1169662475585938, "Std. of action_0 over time_runner": 3.1218042373657227, "Std. of action_1 over agents_runner": 3.148123264312744, "Std. of action_1 over envs_runner": 3.144516706466675, "Std. of action_1 over time_runner": 3.1500115394592285, "Current timestep_runner": 100000.0, "Gradient norm_runner": 0.0, "Mean episodic reward_runner": -437.1199951171875, "epoch": 9, "trainer/global_step": 9, "_timestamp": 1692857347.867884, "_runtime": 200.09672498703003, "_step": 1, "_wandb": {"runtime": 203}}