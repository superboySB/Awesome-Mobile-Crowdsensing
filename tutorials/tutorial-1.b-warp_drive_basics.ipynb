{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.\\\n",
    "All rights reserved.\\\n",
    "SPDX-License-Identifier: BSD-3-Clause\\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/tutorial-1.b-warp_drive_basics.ipynb)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the warp_drive package using\n",
    "\n",
    "- the pip package manager, OR\n",
    "- by cloning the warp_drive package and installing the requirements.\n",
    "\n",
    "We will install the latest version of WarpDrive using the pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U rl_warp_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timeit import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warp_drive.managers.numba_managers.numba_data_manager import NumbaDataManager\n",
    "from warp_drive.managers.numba_managers.numba_function_manager import NumbaFunctionManager\n",
    "from warp_drive.utils.data_feed import DataFeed\n",
    "from warp_drive.utils.common import get_project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will focus on using the Numba backend to run the same content in tutorial 1.a. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will demonstrate how to push and pull data between the host and the device, and how to write simple CUDA functions to manipulate the date. Let's begin by creating a CUDADataManager object. \n",
    "\n",
    "We specify a few multi-agent RL parameters in the `DataManager` creator. \n",
    "\n",
    "We'll create a multi-agent RL environment with 3 agents, an episode length of 5, and 2 environment replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 3\n",
    "num_envs = 2\n",
    "episode_length = 5\n",
    "\n",
    "cuda_data_manager = NumbaDataManager(num_agents, num_envs, episode_length=episode_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some (random) data that we would like to push to the device. In the context of RL, this can pertain to the starting states created by `env reset()`. \n",
    "\n",
    "The starting states are arrays that need to hold data such as observations, actions and rewards during the course of the episode. They could also contain environment configuration settings and hyperparameters. \n",
    "\n",
    "Each environment and agent will have its own data, so we create a `(num_envs, num_agents)`-shaped array that will be pushed to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.random.rand(num_envs, num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push and pull data from host (CPU) to device (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to push data to the device, we have created a **DataFeed** helper object. For all data pushed from the host to device, we will need to provide a name identifier, the actual data, and two flags (both default to False):\n",
    "\n",
    "- `save_copy_and_apply_at_reset` - if `True`, we make a copy of the starting data so that we can set the data array to that value at every environment reset, and\n",
    "- `log_data_across_episode` - if `True`, we add a time dimension to the data, of size `episode_length`, set all $t>0$ index values to zeros, and store the data array at each time step separately. This is primarily used for logging the data for an episode rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feed = DataFeed()\n",
    "data_feed.add_data(\n",
    "    name=\"random_data\",\n",
    "    data=random_data,\n",
    "    save_copy_and_apply_at_reset=False,\n",
    "    log_data_across_episode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA data manager provides the **push_data_to_device()** and **pull_data_from_device()** apis to handle data transfer between the host and the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_data_manager.push_data_to_device(data_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the data manager casted the data from float64 to float32. CUDA always uses 32-bit floating or integer representations of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fetched_from_device = cuda_data_manager.pull_data_from_device(\"random_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data fetched from the device matches the data pushed (the small differences are due to type-casting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fetched_from_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another integral part of RL is training. We also need to hold the observations, actions and rewards arrays. So fo training, we will wrap the data into a Pytorch Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Training Data Accessible To PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pushing and pulling data several times between the host and the device causes a lot of communication overhead. So, it's advisable that we push the data from the host to device only once, and then manipulate all the data on the GPU in-place. This is particularly important when data needs to be accessed frequently. A common example is the batch of observations and rewards gathered for each training iteration. \n",
    "\n",
    "Fortunately, our framework lets Pytorch access the data we pushed onto the GPU via pointers with minimal overhead. To make data accessible by Pytorch, we set the `torch_accessible` flag to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_feed = DataFeed()\n",
    "tensor_feed.add_data(name=\"random_tensor\", data=random_data)\n",
    "\n",
    "cuda_data_manager.push_data_to_device(tensor_feed, torch_accessible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_on_device = cuda_data_manager.data_on_device_via_torch(\"random_tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time comparison for data pull (`torch_accessible` True versus False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_array = np.random.rand(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch_accessible=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feed = DataFeed()\n",
    "data_feed.add_data(\n",
    "    name=\"large_array\",\n",
    "    data=large_array,\n",
    ")\n",
    "cuda_data_manager.push_data_to_device(data_feed, torch_accessible=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Timer(lambda: cuda_data_manager.pull_data_from_device(\"large_array\")).timeit(\n",
    "    number=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch_accessible=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feed = DataFeed()\n",
    "data_feed.add_data(\n",
    "    name=\"large_array_torch\",\n",
    "    data=large_array,\n",
    ")\n",
    "cuda_data_manager.push_data_to_device(data_feed, torch_accessible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Timer(lambda: cuda_data_manager.data_on_device_via_torch(\"random_tensor\")).timeit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the time for accessing torch tensors on the GPU is negligible compared to data arrays!\n",
    "\n",
    "Currently, the `DataManager` supports primitive data types, such as ints, floats, lists, and arrays. If you would like to push more sophisticated data structures or types to the GPU, such as dictionaries, you may do so by pushing / pulling each key-value pair as a separate array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Execution Inside CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we push all the relevant data to the GPU, we will need to write functions to manipulate the data. To this end, we will need to write code in Numba, but invoke it from the host node. The `FunctionManager` is built to facilitate function initialization on the host and execution on the device. As we mentioned before, all the arrays on GPU will be modified on the GPU, and in-place. Let's begin by creating a CUDAFunctionManager object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_function_manager = NumbaFunctionManager(\n",
    "    num_agents=cuda_data_manager.meta_info(\"n_agents\"),\n",
    "    num_envs=cuda_data_manager.meta_info(\"n_envs\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array manipulation inside Numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous tutorial, we have discussed array indexing and our utility functions to facilitate the indexing in CUDA. One great benefit for Numba is its intrisinc syntax for multi-dimensional array indexing. Let's rewrite the same example in Numba this time. To recap, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a simple function to add one to each element of the pushed data. We will perform this operation in parallel on the (num_envs) number of GPU blocks and the (num_agents) number of threads within.\n",
    "\n",
    "In general, the operation is (almost) parallel. Going into a bit more detail - CUDA employs a Single Instruction Multiple Thread (SIMT) architecture to manage and execute threads in groups of 32 called warps. So, as long as the number of agents is a multiple of 32, all the threads ar utilized, otherwise few threads remain idle. For example, if we use $1000$ agents, $24$ threads will remain idle, for a utilization rate of $97.65\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code = \"\"\"\n",
    "import numba.cuda as numba_driver\n",
    "\n",
    "@numba_driver.jit\n",
    "def cuda_increment(data, num_agents):\n",
    "    env_id = numba_driver.blockIdx.x\n",
    "    agent_id = numba_driver.threadIdx.x\n",
    "    if agent_id < num_agents:\n",
    "        increment = env_id + agent_id\n",
    "        data[env_id, agent_id] += increment\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `FunctionManager` API method **import_numba_from_source_code()** to build and load the Numba code.\n",
    "\n",
    "*Note: WarpDrive does not support the direct string-type source code loading. In general, it's standard practice to have several standalone source codes written out in .py file, here, the source_code is saved in example_envs/dummy_env* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_code_path = f\"example_envs.dummy_env.tutorial_basics\"\n",
    "cuda_function_manager.import_numba_from_source_code(\n",
    "    source_code_path, default_functions_included=False\n",
    ")\n",
    "cuda_function_manager.initialize_functions([\"cuda_increment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `FunctionManager`'s API method **get_function()** to load the CUDA kernel function and get an handle to invoke it from the host device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_function = cuda_function_manager.get_function(\"cuda_increment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when invoking the `increment` function, along with the `data` and `num_agents` arguments, we also need to provide the block and grid arguments. These are also attributes of the CUDA `FunctionManager`: simply use\\\n",
    "\n",
    "- `block=cuda_function_manager.block`, and\n",
    "- `grid=cuda_function_manager.grid`\n",
    "\n",
    "Also, since we need to use the `num_agents` parameter, we also need to push it to the device. Instead of using a `DataFeed`, we may also push as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_data_manager.push_data_to_device(\n",
    "    {\n",
    "        \"num_agents\": {\n",
    "            \"data\": num_agents,\n",
    "            \"attributes\": {\n",
    "                \"save_copy_and_apply_at_reset\": False,\n",
    "                \"log_data_across_episode\": False,\n",
    "            },\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block=cuda_function_manager.block\n",
    "grid=cuda_function_manager.grid\n",
    "\n",
    "increment_function[grid, block](\n",
    "    cuda_data_manager.device_data(\"random_data\"),\n",
    "    cuda_data_manager.device_data(\"num_agents\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the original (random) data that we pushed to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here's the incremented data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_data_manager.pull_data_from_device(\"random_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this method incremented each entry at index `(env_id, agent_id)` of the original data by `(env_id + agent_id)`! The differences are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_data_manager.pull_data_from_device(\"random_data\") - random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can invoke the increment function again to increment one more time (also in-place on the GPU), and the differences double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block=cuda_function_manager.block\n",
    "grid=cuda_function_manager.grid\n",
    "\n",
    "increment_function[grid, block](\n",
    "    cuda_data_manager.device_data(\"random_data\"),\n",
    "    cuda_data_manager.device_data(\"num_agents\"),\n",
    "\n",
    ")\n",
    "cuda_data_manager.pull_data_from_device(\"random_data\") - random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating CUDA parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put all the pieces introduced so far together, and record the times for parallelized operations with different `num_envs` and `num_agents` settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_random_data_and_increment_timer(\n",
    "    num_runs=1,\n",
    "    num_envs=2,\n",
    "    num_agents=3,\n",
    "    source_code_path=None,\n",
    "    episode_length=100,\n",
    "):\n",
    "\n",
    "    assert source_code_path is not None\n",
    "\n",
    "    # Initialize the CUDA data manager\n",
    "    cuda_data_manager = NumbaDataManager(\n",
    "        num_agents=num_agents, num_envs=num_envs, episode_length=episode_length\n",
    "    )\n",
    "\n",
    "    # Initialize the CUDA function manager\n",
    "    cuda_function_manager = NumbaFunctionManager(\n",
    "        num_agents=cuda_data_manager.meta_info(\"n_agents\"),\n",
    "        num_envs=cuda_data_manager.meta_info(\"n_envs\"),\n",
    "    )\n",
    "\n",
    "    # Load source code and initialize function\n",
    "    cuda_function_manager.import_numba_from_source_code(\n",
    "    source_code_path, default_functions_included=False\n",
    ")\n",
    "    cuda_function_manager.initialize_functions([\"cuda_increment\"])\n",
    "    increment_function = cuda_function_manager.get_function(\"cuda_increment\")\n",
    "\n",
    "    def push_random_data(num_agents, num_envs):\n",
    "        # Create random data\n",
    "        random_data = np.random.rand(num_envs, num_agents)\n",
    "\n",
    "        # Push data from host to device\n",
    "        data_feed = DataFeed()\n",
    "        data_feed.add_data(\n",
    "            name=\"random_data\",\n",
    "            data=random_data,\n",
    "        )\n",
    "        data_feed.add_data(name=\"num_agents\", data=num_agents)\n",
    "        cuda_data_manager.push_data_to_device(data_feed)\n",
    "\n",
    "    def increment_data():\n",
    "        block=cuda_function_manager.block\n",
    "        grid=cuda_function_manager.grid\n",
    "        \n",
    "        increment_function[grid, block](\n",
    "            cuda_data_manager.device_data(\"random_data\"),\n",
    "            cuda_data_manager.device_data(\"num_agents\"),\n",
    "        )\n",
    "\n",
    "    # One-time data push\n",
    "    data_push_time = Timer(lambda: push_random_data(num_agents, num_envs)).timeit(\n",
    "        number=1\n",
    "    )\n",
    "    # Increment the arrays 'num_runs' times\n",
    "    program_run_time = Timer(lambda: increment_data()).timeit(number=num_runs)\n",
    "\n",
    "    return {\"data push times\": data_push_time, \"code run time\": program_run_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the times for a single data push and 10000 increment kernel calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "num_runs = 10000\n",
    "times = {}\n",
    "\n",
    "for scenario in [\n",
    "    (1, 1),\n",
    "    (1, 10),\n",
    "    (1, 100),\n",
    "    (10, 10),\n",
    "    (1, 1000),\n",
    "    (100, 100),\n",
    "    (1000, 1000),\n",
    "]:\n",
    "    num_envs, num_agents = scenario\n",
    "    times.update(\n",
    "        {\n",
    "            f\"envs={num_envs}, agents={num_agents}\": push_random_data_and_increment_timer(\n",
    "                num_runs, num_envs, num_agents, source_code_path\n",
    "            )\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Times for {num_runs} function calls\")\n",
    "print(\"*\" * 40)\n",
    "for key, value in times.items():\n",
    "    print(\n",
    "        f\"{key:30}: data push time: {value['data push times']:10.5}s,\\t mean increment times: {value['code run time']:10.5}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the number of environments and agents, the data size becomes larges, so pushing data becomes slower, but since all the threads operate in parallel, the average time taken in the increment function remains about the same!\n",
    "\n",
    "Also notice that Numba is much slower (~1/10X) than PyCUDA in this simple example. The main reason is that JIT will repeat its runtime compilation everytime when it is being called. Since the execution of the kernel function is pretty lightweight in this example, the compilation time actually dominates the time. This problem will be improved much in the real problem when the kernel function itself takes much more time and JIT will also help to optimize the kernel execution at the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! By using building blocks such as the increment function, we can create arbitrarily complex functions in CUDA C. For some comparative examples, please see the example environments that have both Python implementations in `examples/envs` and corresponding CUDA C implementations in `src/envs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some useful starting resources for CUDA C programming:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [CUDA tutorial](https://cuda-tutorial.readthedocs.io/en/latest/)\n",
    "- [Learn C](https://learnxinyminutes.com/docs/c/)\n",
    "- [CUDA Quick Reference](http://www.icl.utk.edu/~mgates3/docs/cuda.html)\n",
    "<!-- - [Thrust](https://developer.nvidia.com/thrust). Note: thrust is a flexible, high-level interface for GPU programming that greatly enhances developer productivity. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn More and Explore our Tutorials!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first tutorial on WarpDrive. Next, we suggest you check out our advanced tutorials on [WarpDrive's sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb) and [WarpDrive's reset and log controller](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb).\n",
    "\n",
    "For your reference, all our tutorials are here:\n",
    "1. [WarpDrive basics(intro and pycuda)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1.a-warp_drive_basics.ipynb)\n",
    "2. [WarpDrive basics(numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1.b-warp_drive_basics.ipynb)\n",
    "3. [WarpDrive sampler(pycuda)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2.a-warp_drive_sampler.ipynb)\n",
    "4. [WarpDrive sampler(numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2.b-warp_drive_sampler.ipynb)\n",
    "5. [WarpDrive resetter and logger](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "6. [Create custom environments (pycuda)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4.a-create_custom_environments_pycuda.md)\n",
    "7. [Create custom environments (numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4.b-create_custom_environments_numba.md)\n",
    "8. [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)\n",
    "9. [Scaling Up training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md)\n",
    "10. [Training with WarpDrive + Pytorch Lightning](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
