{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc. \\\n",
    "All rights reserved. \\\n",
    "SPDX-License-Identifier: BSD-3-Clause \\\n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.device_count() > 0, \"This notebook needs a GPU to run!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to WarpDrive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our third (and an advanced) tutorial about WarpDrive, a framework for extremely parallelized multi-agent reinforcement learning (RL) on a single GPU. If you haven't yet, please also checkout our previous tutorials\n",
    "\n",
    "- WarpDrive basics\n",
    "  - [Introduction and PyCUDA](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1.a-warp_drive_basics.ipynb)\n",
    "  - [Numba](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1.b-warp_drive_basics.ipynb)\n",
    "- WarpDrive sampler\n",
    "  - [PyCUDA](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2.a-warp_drive_sampler.ipynb)\n",
    "  - [Numba](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2.b-warp_drive_sampler.ipynb)\n",
    "\n",
    "In this tutorial, we describe **PyCUDAEnvironmentReset** and **PyCUDALogController**. \n",
    "\n",
    "- PyCUDAEnvironmentReset works exclusively on the GPU to reset the environment in-place. \n",
    "- PyCUDALogController works exclusively in the GPU device to log the episode history. \n",
    "\n",
    "They both play important roles in the WarpDrive framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the warp_drive package using\n",
    "\n",
    "- the pip package manager, OR\n",
    "- by cloning the warp_drive package and installing the requirements.\n",
    "\n",
    "On Colab, we will do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    ! git clone https://github.com/salesforce/warp-drive.git\n",
    "    % cd warp-drive\n",
    "    ! pip install -e .\n",
    "else:\n",
    "    ! pip install -U rl_warp_drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from warp_drive.managers.pycuda_managers.pycuda_data_manager import PyCUDADataManager\n",
    "from warp_drive.managers.pycuda_managers.pycuda_function_manager import (\n",
    "    PyCUDAFunctionManager,\n",
    "    PyCUDALogController,\n",
    "    PyCUDAEnvironmentReset,\n",
    ")\n",
    "from warp_drive.utils.constants import Constants\n",
    "from warp_drive.utils.data_feed import DataFeed\n",
    "from warp_drive.utils.common import get_project_root\n",
    "\n",
    "_MAIN_FILEPATH = f\"{get_project_root()}/warp_drive/cuda_includes\"\n",
    "_CUBIN_FILEPATH = f\"{get_project_root()}/warp_drive/cuda_bin\"\n",
    "_ACTIONS = Constants.ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDAEnvironmentReset and CUDALogController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have developed a CUDA environment `step` function, here we show how WarpDrive can help to facilitate the environment rollout by resetting and logging the environment on the GPU. If you do not have \"test_build.cubin\" built, you can refer to the previous tutorial [WarpDrive sampler](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2-warp_drive_sampler.ipynb) about how to automatically build it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _log_mask_                                                                      : dtype=int32     , shape=(3,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _done_                                                                          : dtype=int32     , shape=(2,)\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "INFO:root:- _timestep_                                                                      : dtype=int32     , shape=(2,)\n"
     ]
    }
   ],
   "source": [
    "cuda_data_manager = PyCUDADataManager(num_agents=5, num_envs=2, episode_length=2)\n",
    "cuda_function_manager = PyCUDAFunctionManager(\n",
    "    num_agents=cuda_data_manager.meta_info(\"n_agents\"),\n",
    "    num_envs=cuda_data_manager.meta_info(\"n_envs\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully mkdir the binary folder /export/home/codebase/warp-drive/warp_drive/cuda_bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/export/home/codebase/warp-drive/warp_drive/cuda_includes/../../example_envs/tag_gridworld/tag_gridworld_step.cu(151): warning: invalid narrowing conversion from \"unsigned int\" to \"int\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running cmd: nvcc --fatbin -arch=sm_80 /export/home/codebase/warp-drive/warp_drive/cuda_includes/test_build.cu -o /export/home/codebase/warp-drive/warp_drive/cuda_bin/test_build.fatbin\n",
      "INFO:root:Successfully build the cubin_file from /export/home/codebase/warp-drive/warp_drive/cuda_includes/test_build.cu to /export/home/codebase/warp-drive/warp_drive/cuda_bin/test_build.fatbin\n"
     ]
    }
   ],
   "source": [
    "main_example_file = f\"{_MAIN_FILEPATH}/test_build.cu\"\n",
    "bin_example_file = f\"{_CUBIN_FILEPATH}/test_build.fatbin\"\n",
    "\n",
    "cuda_function_manager._compile(main_file=main_example_file, \n",
    "                               cubin_file=bin_example_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully load the cubin_file from /export/home/codebase/warp-drive/warp_drive/cuda_bin/test_build.fatbin\n",
      "INFO:root:starting to load the cuda kernel function: reset_log_mask from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_log_mask from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: update_log_mask from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: update_log_mask from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: log_one_step_in_float from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: log_one_step_in_float from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: log_one_step_in_int from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: log_one_step_in_int from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: reset_in_float_when_done_2d from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_in_float_when_done_2d from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: reset_in_int_when_done_2d from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_in_int_when_done_2d from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: reset_in_float_when_done_3d from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_in_float_when_done_3d from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: reset_in_int_when_done_3d from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: reset_in_int_when_done_3d from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: undo_done_flag_and_reset_timestep from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: undo_done_flag_and_reset_timestep from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: init_random from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: init_random from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: free_random from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: free_random from the CUDA module, \n",
      "INFO:root:starting to load the cuda kernel function: sample_actions from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: sample_actions from the CUDA module, \n",
      "INFO:root:Successfully initialize the default CUDA functions managed by the CUDAFunctionManager\n"
     ]
    }
   ],
   "source": [
    "cuda_function_manager.load_cuda_from_binary_file(bin_example_file)\n",
    "cuda_env_resetter = PyCUDAEnvironmentReset(function_manager=cuda_function_manager)\n",
    "cuda_env_logger = PyCUDALogController(function_manager=cuda_function_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an example step function already checked in and compiled inside `test_build.cubin`. \n",
    "\n",
    "The source code of this dummy step function can be found [here](https://www.github.com/salesforce/warp-drive/blob/master/example_envs/dummy_env/test_step.cu). For each step, array `x` will be divided by `multiplier` while array `y` will be multiplied by the same `multiplier`:\n",
    "\n",
    "```\n",
    "x[index] = x[index] / multiplier;\n",
    "y[index] = y[index] * multiplier;\n",
    "```\n",
    "\n",
    "Now we just need to initialize it with CUDAFunctionManager and wrap up it with a Python/CUDA step callable. In `dummy_env` this function is called `cuda_dummy_step()`. \n",
    "\n",
    "Notice that we provide the **EnvWrapper** to wrap up most of processes below automatically. However, the unique Python/CUDA step callable you developed needs to be defined inside your environment so **EnvWrapper** can find and wrap it up. \n",
    "\n",
    "For concrete examples on how to define more complex `step` functions, you can refer to [example1](https://www.github.com/salesforce/warp-drive/blob/master/example_envs/tag_gridworld/tag_gridworld_step.cu) and [example2](https://www.github.com/salesforce/warp-drive/blob/master/example_envs/tag_continous/tag_continuous_step.cu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:starting to load the cuda kernel function: testkernel from the CUDA module \n",
      "INFO:root:finished loading the cuda kernel function: testkernel from the CUDA module, \n"
     ]
    }
   ],
   "source": [
    "cuda_function_manager.initialize_functions([\"testkernel\"])\n",
    "\n",
    "\n",
    "def cuda_dummy_step(\n",
    "    function_manager: PyCUDAFunctionManager,\n",
    "    data_manager: PyCUDADataManager,\n",
    "    env_resetter: PyCUDAEnvironmentReset,\n",
    "    target: int,\n",
    "    step: int,\n",
    "):\n",
    "\n",
    "    env_resetter.reset_when_done(data_manager)\n",
    "\n",
    "    step = np.int32(step)\n",
    "    target = np.int32(target)\n",
    "    test_step = function_manager.get_function(\"testkernel\")\n",
    "    test_step(\n",
    "        data_manager.device_data(\"X\"),\n",
    "        data_manager.device_data(\"Y\"),\n",
    "        data_manager.device_data(\"_done_\"),\n",
    "        data_manager.device_data(f\"{_ACTIONS}\"),\n",
    "        data_manager.device_data(\"multiplier\"),\n",
    "        target,\n",
    "        step,\n",
    "        data_manager.meta_info(\"episode_length\"),\n",
    "        block=function_manager.block,\n",
    "        grid=function_manager.grid,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset and Log Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `step` function above, besides the step function managed by CUDAFunctionManager, you can see the function called `CUDAEnvironmentReset.reset_when_done()`. This function will reset the corresponding env to its initial state when the `done` flag becomes true on the GPU. This reset only resets the env that is done. \n",
    "\n",
    "To make it work properly, you need to specify which data (usually the feature arrays and observations) can be reset. \n",
    "\n",
    "This is where the flag **save_copy_and_apply_at_reset** comes into play. If the data has `save_copy_and_apply_at_reset` set to True, a dedicated copy will be maintained in the device for resetting. \n",
    "\n",
    "On the other hand, **log_data_across_episode** will create a buffer on the GPU for logs. This lets you record a complete episode. \n",
    "\n",
    "These two functions can be independently used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:PyCUDADataManager casts the data 'X' from type float64 to float32\n",
      "INFO:root:- X                                                                               : dtype=float32   , shape=(2, 5)\n",
      "INFO:root:- X_at_reset                                                                      : dtype=float32   , shape=(2, 5)\n",
      "INFO:root:- X_for_log                                                                       : dtype=float32   , shape=(3, 5)\n",
      "WARNING:root:PyCUDADataManager casts the data 'Y' from type int64 to int32\n",
      "INFO:root:- Y                                                                               : dtype=int32     , shape=(2, 5)\n",
      "INFO:root:- Y_at_reset                                                                      : dtype=int32     , shape=(2, 5)\n",
      "INFO:root:- Y_for_log                                                                       : dtype=int32     , shape=(3, 5)\n",
      "INFO:root:- multiplier                                                                      : dtype=float32   , shape=()\n",
      "INFO:root:\n",
      "Pushing data to device...\n",
      "WARNING:root:PyCUDADataManager casts the data 'sampled_actions' from type int64 to int32\n",
      "INFO:root:- sampled_actions                                                                 : dtype=int32     , shape=(2, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "data = DataFeed()\n",
    "data.add_data(\n",
    "    name=\"X\",\n",
    "    data=[[0.1, 0.2, 0.3, 0.4, 0.5], [0.6, 0.7, 0.8, 0.9, 1.0]],\n",
    "    save_copy_and_apply_at_reset=True,\n",
    "    log_data_across_episode=True,\n",
    ")\n",
    "\n",
    "data.add_data(\n",
    "    name=\"Y\",\n",
    "    data=np.array([[6, 7, 8, 9, 10], [1, 2, 3, 4, 5]]),\n",
    "    save_copy_and_apply_at_reset=True,\n",
    "    log_data_across_episode=True,\n",
    ")\n",
    "data.add_data(name=\"multiplier\", data=2.0)\n",
    "\n",
    "tensor = DataFeed()\n",
    "tensor.add_data(\n",
    "    name=f\"{_ACTIONS}\",\n",
    "    data=[\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    ],\n",
    ")\n",
    "\n",
    "cuda_data_manager.push_data_to_device(data)\n",
    "cuda_data_manager.push_data_to_device(tensor, torch_accessible=True)\n",
    "\n",
    "assert cuda_data_manager.is_data_on_device(\"X\")\n",
    "assert cuda_data_manager.is_data_on_device(\"Y\")\n",
    "assert cuda_data_manager.is_data_on_device_via_torch(f\"{_ACTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run an complete set of parallel episodes and inspect the log for the first environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:reset log for env 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The done array = [1 1]\n"
     ]
    }
   ],
   "source": [
    "# t = 0 is reserved for the initial state.\n",
    "cuda_env_logger.reset_log(data_manager=cuda_data_manager, env_id=0)\n",
    "\n",
    "for t in range(1, cuda_data_manager.meta_info(\"episode_length\") + 1):\n",
    "    cuda_dummy_step(\n",
    "        function_manager=cuda_function_manager,\n",
    "        data_manager=cuda_data_manager,\n",
    "        env_resetter=cuda_env_resetter,\n",
    "        target=100,\n",
    "        step=t,\n",
    "    )\n",
    "    cuda_env_logger.update_log(data_manager=cuda_data_manager, step=t)\n",
    "\n",
    "dense_log = cuda_env_logger.fetch_log(data_manager=cuda_data_manager, names=[\"X\", \"Y\"])\n",
    "\n",
    "# Test after two steps that the log buffers for X and Y log are updating.\n",
    "X_update = dense_log[\"X_for_log\"]\n",
    "Y_update = dense_log[\"Y_for_log\"]\n",
    "\n",
    "assert abs(X_update[1].mean() - 0.15) < 1e-5\n",
    "assert abs(X_update[2].mean() - 0.075) < 1e-5\n",
    "assert Y_update[1].mean() == 16\n",
    "assert Y_update[2].mean() == 32\n",
    "\n",
    "# Right now, the reset functions have not been activated.\n",
    "# The done flags should be all True now.\n",
    "\n",
    "done = cuda_data_manager.pull_data_from_device(\"_done_\")\n",
    "print(f\"The done array = {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we can explicitly reset the environment to see how it works. The `dummy_step` function will do this in the next step by itself as well. After resetting, you can see that all the done flags go back to False and the `X` and `Y` arrays get reset successfully as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_env_resetter.reset_when_done(data_manager=cuda_data_manager)\n",
    "\n",
    "done = cuda_data_manager.pull_data_from_device(\"_done_\")\n",
    "assert done[0] == 0\n",
    "assert done[1] == 0\n",
    "\n",
    "X_after_reset = cuda_data_manager.pull_data_from_device(\"X\")\n",
    "Y_after_reset = cuda_data_manager.pull_data_from_device(\"Y\")\n",
    "# the 0th dim is env\n",
    "assert abs(X_after_reset[0].mean() - 0.3) < 1e-5\n",
    "assert abs(X_after_reset[1].mean() - 0.8) < 1e-5\n",
    "assert Y_after_reset[0].mean() == 8\n",
    "assert Y_after_reset[1].mean() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn More and Explore our Tutorials!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have familiarized yourself with WarpDrive, we suggest you take a look at our tutorials on [creating custom environments](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md) and on how to use WarpDrive to perform end-to-end multi-agent reinforcement learning [training](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)!\n",
    "\n",
    "For your reference, all our tutorials are here:\n",
    "1. [WarpDrive basics(intro and pycuda)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1.a-warp_drive_basics.ipynb)\n",
    "2. [WarpDrive basics(numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-1.b-warp_drive_basics.ipynb)\n",
    "3. [WarpDrive sampler(pycuda)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2.a-warp_drive_sampler.ipynb)\n",
    "4. [WarpDrive sampler(numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-2.b-warp_drive_sampler.ipynb)\n",
    "5. [WarpDrive resetter and logger](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-3-warp_drive_reset_and_log.ipynb)\n",
    "6. [Create custom environments (pycuda)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4.a-create_custom_environments_pycuda.md)\n",
    "7. [Create custom environments (numba)](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4.b-create_custom_environments_numba.md)\n",
    "8. [Training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-5-training_with_warp_drive.ipynb)\n",
    "9. [Scaling Up training with WarpDrive](https://www.github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-6-scaling_up_training_with_warp_drive.md)\n",
    "10. [Training with WarpDrive + Pytorch Lightning](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-7-training_with_warp_drive_and_pytorch_lightning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
